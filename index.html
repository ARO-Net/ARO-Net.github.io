<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Learning Implicit Fields from Anchored Radial Observations.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ARO-Net</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-abc-8031.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-abc-8031.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://yizhiwang96.github.io/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">ARO-Net: Learning Implicit Fields from Anchored Radial Observations</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://yizhiwang96.github.io">Yizhi Wang*</a><sup>1,2</sup>,</span>
            <span class="author-block">
              <a href="https://github.com/zzilch">Zeyu Huang*</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://faculty.runi.ac.il/arik/site/index.asp">Ariel Shamir</a><sup>3</sup>,</span>
            <span class="author-block">
              <a href="https://vcc.tech/~huihuang">Hui Huang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.cs.sfu.ca/~haoz/">Hao Zhang</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://csse.szu.edu.cn/staff/ruizhenhu/">Ruizhen Hu</a><sup>1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Shenzhen University,</span>
            <span class="author-block"><sup>2</sup>Simon Fraser University</span>
            <span class="author-block"><sup>3</sup>Reichman University</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2212.10275"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2212.10275"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=RVoOkgbi9lk&t=49s&ab_channel=YizhiWang"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/yizhiwang96/ARO-Net"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>

            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="columns is-centered has-text-centered">
        <video width=50% height=50% id="teaser" autoplay muted loop playsinline>
          <source src="./static/videos/teaser.mp4"
                  type="video/mp4">
        </video>
      </div>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">ARO</span> is a new representation for shapes based on radial observations from a set of viewpoints.
      </h2>
    </div>
  </div>
</section>


<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-abc-8031" class="abc-8031 results-abc-8031">
        <div class="item item-steve">
          <img src="./static/gifs/animation_abc_8031.gif" alt="abc-8031">
        </div>
      </div>
      <p>
        ARO-Net encodes <b>contextual</b> information from anchors, which is different against most existing methods (such as <a href="https://github.com/ErlerPhilipp/points2surf">Points2Surf</a> and <a href="https://pengsongyou.github.io/conv_onet">ConvONet</a>) that encode <b>neighboring</b> information around query point, yielding better reconstruction (see the holes) on sparse input and generalizability on unseen categories.
      </p>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We introduce anchored radial observations (ARO), a novel shape encoding for learning implicit field representation of 3D shapes that is category-agnostic and generalizable amid significant shape variations.
          </p>
          <p>
            The main idea behind our work is to reason about shapes through partial observations from a set of viewpoints, called anchors. We develop a general and unified shape representation by employing a fixed set of anchors, via Fibonacci sampling, and designing a coordinate-based deep neural network to predict the occupancy value of a query point in space. Differently from prior neural implicit models that use global shape feature, our shape encoder operates on contextual, query-specific features. To predict point occupancy, locally observed shape information from the perspective of the anchors surrounding the input query point are encoded and aggregated through an attention module, before implicit decoding is performed.
          </p>
          <p>
            We demonstrate the quality and generality of our network, coined ARO-Net, on surface reconstruction from sparse point clouds, with tests on novel and unseen object categories, "one-shape" training, and comparisons to state-of-the-art neural and classical methods for reconstruction and tessellation.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/RVoOkgbi9lk?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    <!--/ Paper video. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">How does ARO work?</h2>

        <div class="content has-text-justified">
          <img src="./static/images/vis_and_occ.png" alt="vis_and_occ">
          <p>
            If we have a set of anchors that together observe the entire surface of the shape, whether a query point is inside/outside can be determined by its relationship to the radial observations of anchors. 
            For example, in subfigure (b) when anchors are all interior, 
          </p>
          <div class="columns is-centered has-text-centered">
            <p>
            a query point (red star) is inside the shape 
            â‡”
            it is covered by at least one radial observation (the red anchor)
           </p>
          </div>
            (For other situations, please check our video and paper for more details.)
          <p>
            ARO learns an <a href="https://github.com/vsitzmann/awesome-implicit-representations#what-are-implicit-neural-representations">implicit function</a> by modeling the observations from different anchors to query point, which is <b>contextual</b> and <b>query-specific</b>, making it quite different from existing methods.
          </p>
          </div>

        <br/>


      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">ARO Network</h2>

        <div class="content has-text-justified">
          <img src="./static/images/pipeline.png" alt="pipeline">
          <p>
            In practice, shapes can only be <b>partially</b> observed by a set of anchors. We introduce ARO-Net to predict the occupancy from less/incomplete Information after training with data. ARO-Net learns to predict the occupancy of a query point given only partial observations.
        
          </div>

        <br/>


      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Anchor Placement</h2>

        <div class="content has-text-justified">
          <div class="columns is-centered has-text-centered">
            <img src="./static/images/anchor_placement.png" alt="anchor_placement" width="50%">
          </div>
          <p>
            We use <a href="http://extremelearning.com.au/how-to-evenly-distribute-points-on-a-sphere-more-effectively-than-the-canonical-fibonacci-lattice/">Fibonacci Sampling</a> to construct the anchors. Layered Fibonacci sampling makes the anchors have generally good visibility about the shapes.
          </p>
        </div>

        <br/>


      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Strong Generalizability</h2>
        <div class="columns is-centered has-text-centered">
        <img src="./static/images/generality.png" alt="generality" width="70%">
        </div>
        <div class="content has-text-justified">
          <p>
            A somewhat extreme toy example comparing ARO-Net to prior occupancy prediction networks on 3D reconstruction from a sparse point cloud of a cube (a), with training on a single sphere.
          </p>
        </div>

        <br/>


      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Better Reconstruction of Details</h2>

        <div class="content has-text-justified">
          <p>
            Visually, ARONet produces the most complete, artifact-free results with faithful reconstruction of fine details. In this airplane case, ARO-Net managed to reconstruct the left-side turbine from extremely sparse points, while other methods all completely missed it.
          </p>
          <div class="item airplane_7">
            <img src="./static/gifs/animation_airplane_7_view1.gif" alt="airplane_7_view1">
          </div>  
          <div class="item airplane_7">
            <img src="./static/gifs/animation_airplane_7_view2.gif" alt="airplane_7_view2">
          </div>  
        </div>

        <br/>


      </div>
    </div>
    <!--/ Matting. -->

    <!-- Animation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Robustness to Sparsity</h2>

        <div class="content has-text-justified">
          <p>
            ARO-Net exhibits superior robustness to sparsity of input point clouds compared to its close competitors.
          </p>

          <p>
            When all methids are trained on 2048-point input and tested on 2048-point input, ARO achieves the best reconstruction quality while results from others are also okay.
          </p>
          <div class="item abc_8026">
            <img src="./static/gifs/animation_abc_8260_pcd_2048.gif" alt="abc-8026-2048">
          </div>            
          <p>
            However, when testing on 1024 and 512 points as input, ARO-Net performs much better than other SOTA methods.
          </p>
          <div class="item abc_8026">
            <img src="./static/gifs/animation_abc_8260_pcd_1024.gif" alt="abc-8026-1024">
          </div>   
          <div class="item abc_8026">
            <img src="./static/gifs/animation_abc_8260_pcd_512.gif" alt="abc-8026-512">
          </div>

        </div>

        <br/>


      </div>
    </div>
    <!--/ Animation. -->



    <!--/ Concurrent Work. -->

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{wang2023aro,
  author    = {Wang, Yizhi and Huang, Zeyu and Shamir, Ariel and Huang, Hui and Zhang, Hao and Hu, Ruizhen},
  title     = {ARO-Net: Learning Implicit Fields from Anchored Radial Observations},
  journal   = {CVPR},
  year      = {2023},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
